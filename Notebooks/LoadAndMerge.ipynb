{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f7f9fb",
   "metadata": {},
   "source": [
    "# Read Multiple Data Files, Transform and Write into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c3104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below line to Install any required packages\n",
    "# !pip install tqdm h5py natsort pympler hdf5storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9294d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from pympler import asizeof\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from collections import OrderedDict\n",
    "import h5py\n",
    "from datetime import datetime\n",
    "from natsort import natsorted\n",
    "import gzip\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99419483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the dataset\n",
    "datasetspath = \"/Users/saransathy/WalshDBA/Capstone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55640f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Raw_old.mat file to MAT 7.3 format\n",
    "#import scipy.io\n",
    "#import hdf5storage\n",
    "#import os\n",
    "\n",
    "# Rename the Folder16 file in Drive & convert to MAT 7.3 format for uniformity\n",
    "# os.rename(f\"{datasetspath}/Drive/Folder16/Raw.mat\", f\"{datasetspath}/Drive/Folder16/Raw_old.mat\")\n",
    "\n",
    "# Step 1: Load the .mat file using scipy.io\n",
    "#data = scipy.io.loadmat(f\"{datasetspath}/Drive/Folder16/Raw_old.mat\")\n",
    "\n",
    "# Step 2: Remove internal keys added by scipy\n",
    "#data_clean = {k: v for k, v in data.items() if not k.startswith('__')}\n",
    "\n",
    "# Step 3: Save as .mat v7.3 (HDF5) file\n",
    "#hdf5storage.savemat(f\"{datasetspath}/Drive/Folder16/Raw.mat\", data_clean, format='7.3')\n",
    "\n",
    "# Step 4: Cleanup the variables\n",
    "#del data, data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f615f7",
   "metadata": {},
   "source": [
    "## Read & Load Audi eTron Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b1f9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract data from the h5py file\n",
    "def extract_group(f, path):\n",
    "    if isinstance(f[path], h5py.Group):\n",
    "        return {k: extract_group(f, f\"{path}/{k}\") for k in f[path].keys()}\n",
    "    else:\n",
    "        return np.array(f[path])\n",
    "    \n",
    "# Function to identify series break and return corresponding epoc\n",
    "def series_break(data, timeepoch):\n",
    "    prev = data[0]\n",
    "    i = 1\n",
    "    ti = 0\n",
    "    idxlist = [(0,0)]\n",
    "    while i < len(data):\n",
    "        if (prev > data[i]):\n",
    "            while ti < len(timeepoch):\n",
    "                if (prev < timeepoch[ti]):\n",
    "                    idxlist.append((i,ti))\n",
    "                    break\n",
    "                ti+=1\n",
    "        prev = data[i]\n",
    "        i += 1\n",
    "    idxlist.append((i, len(timeepoch)))\n",
    "    #print(\"Total Sets: \",idxlist)\n",
    "    return(idxlist)\n",
    "    \n",
    "def merge_etron_data_files(path):\n",
    "    # Process the data files\n",
    "    data_files = natsorted(glob(f\"{datasetspath}/{path}/*/Raw.mat\"))\n",
    "    #data_files = [f\"{datasetspath}/Drive/Folder12/Raw.mat\"]\n",
    "    print(f\"Reading and Processing Data files from {datasetspath}/{path}/\")\n",
    "    dataset = OrderedDict((key,[]) for key in ['TimeCurr','Curr','TimeVolt','Volt','TimeSoC','SoC','TimeTemp','Temp'])\n",
    "    for each_path in tqdm(data_files):\n",
    "        # Load the .mat file\n",
    "        with h5py.File(each_path, 'r') as file:\n",
    "            # Extract the relevant data\n",
    "            data = extract_group(file, 'Raw')\n",
    "        \n",
    "        for key in ['Curr','Volt','SoC','Temp']:\n",
    "            idxs = series_break(data[f\"Time{key}\"][0],data['TimeEpoch'][0])\n",
    "            i = 0\n",
    "            while i < len(idxs)-1:\n",
    "                fdi, fti = idxs[i]\n",
    "                ldi, lti = idxs[i+1]\n",
    "                dataset[key].extend(data[key][0][fdi:ldi])\n",
    "                dataset[f\"Time{key}\"].extend(data[f\"Time{key}\"][0][fdi:ldi]+data['Epoch'][0][fti])\n",
    "                i = i+1\n",
    "                    \n",
    "        del data\n",
    "    print(f\"Merging and Writing data to {datasetspath}/{path}.pkl\")\n",
    "    dfdict = {}\n",
    "    for key in dataset.keys():\n",
    "        if (key.startswith('Time')):\n",
    "            continue\n",
    "        timekey = f\"Time{key}\"\n",
    "        df = pd.DataFrame({timekey: dataset[timekey], key: dataset[key]})\n",
    "        df[timekey] = pd.to_datetime(df[timekey],unit='s')\n",
    "        df.set_index(timekey, inplace=True)\n",
    "        dfdict[key] = df\n",
    "        del df,timekey\n",
    "    #data_series = {k: pd.Series(v, name=k) for k, v in dataset.items()}\n",
    "    pd.to_pickle(dfdict, f\"{datasetspath}/{path}.pkl\")\n",
    "    del data_files, dfdict\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a0f3fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and Processing Data files from /Users/saransathy/WalshDBA/Capstone/Charge/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:38<00:00,  4.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging and Writing data to /Users/saransathy/WalshDBA/Capstone/Charge.pkl\n",
      "Reading and Processing Data files from /Users/saransathy/WalshDBA/Capstone/Drive/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:14<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging and Writing data to /Users/saransathy/WalshDBA/Capstone/Drive.pkl\n"
     ]
    }
   ],
   "source": [
    "# Process and Merge data files into Pandas Data Series\n",
    "merge_etron_data_files('Charge')\n",
    "merge_etron_data_files('Drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062475e6",
   "metadata": {},
   "source": [
    "## Read and Load Multivehicle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a893831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read multivehicle data files\n",
    "def merge_mvdata_files(path):\n",
    "    \n",
    "    data_files = natsorted(glob(f\"{datasetspath}/{path}/data/*.pkl\"))\n",
    "    print(f\"Reading and Processing Data files from {datasetspath}/{path}/\")\n",
    "    \n",
    "    #  Initialize Keys\n",
    "    metakeys = ['label','mileage','capacity','car','charge_segment']\n",
    "    ckeys = ['Curr','SoC','Temp','Volt','MaxVolt','MinVolt','MinTemp']\n",
    "    ckeysmap = {\"Volt\":0, \"Curr\":1, \"MaxVolt\":2, \"MinVolt\":3, \"MinTemp\":4, \"Temp\":5, \"SoC\":6}\n",
    "    \n",
    "    datadict = OrderedDict((key,[]) for key in metakeys+ckeys)\n",
    "    \n",
    "    for each_path in tqdm(data_files):\n",
    "\n",
    "        # Load the pickle file\n",
    "        # Use 'rb' mode to read the file in binary format\n",
    "        with open(each_path, 'rb') as file:\n",
    "            # Load the data using torch.load\n",
    "            # Set weights_only=False to load the entire file\n",
    "            this_pkl_file = torch.load(file, weights_only=False)\n",
    "\n",
    "        # Assuming the pickle file contains a tuple with data and metadata\n",
    "        metadata = this_pkl_file[1]\n",
    "        timedata = this_pkl_file[0]\n",
    "\n",
    "        # Append data to the datadict\n",
    "        for key in metakeys:\n",
    "            datadict[key].extend([metadata[key]] * 128)\n",
    "        for key in ckeys:\n",
    "            datadict[key].extend(timedata[:, ckeysmap[key]].tolist())\n",
    "\n",
    "        # Release the memory\n",
    "        del this_pkl_file,metadata,timedata\n",
    "\n",
    "    # Write to pickle file\n",
    "    print(f\"Merging and Writing data to {datasetspath}/{path}.pkl\")\n",
    "    df = pd.DataFrame(datadict)\n",
    "    df.to_pickle(f\"{datasetspath}/{path}.pkl\")\n",
    "    del data_files,datadict,df\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e88b0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and Processing Data files from /Users/saransathy/WalshDBA/Capstone/battery_dataset1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 629121/629121 [03:21<00:00, 3125.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging and Writing data to /Users/saransathy/WalshDBA/Capstone/battery_dataset1.pkl\n",
      "Reading and Processing Data files from /Users/saransathy/WalshDBA/Capstone/battery_dataset2/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 472829/472829 [02:00<00:00, 3926.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging and Writing data to /Users/saransathy/WalshDBA/Capstone/battery_dataset2.pkl\n",
      "Reading and Processing Data files from /Users/saransathy/WalshDBA/Capstone/battery_dataset3/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176327/176327 [00:38<00:00, 4593.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging and Writing data to /Users/saransathy/WalshDBA/Capstone/battery_dataset3.pkl\n"
     ]
    }
   ],
   "source": [
    "merge_mvdata_files('battery_dataset1')\n",
    "merge_mvdata_files('battery_dataset2')\n",
    "merge_mvdata_files('battery_dataset3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_env]",
   "language": "python",
   "name": "conda-env-tf_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
